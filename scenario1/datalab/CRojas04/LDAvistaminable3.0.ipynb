{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smart-open\n",
    "!pip install --upgrade gensim   \n",
    "!pip install nltk\n",
    "!pip install pyLDAvis\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings en nltk: tokenizador y stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words_nltk = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la ruta del archivo indiferente del sistema operativo\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=\"C:\\\\Users\\\\erojas\\\\Desktop\\\\Downloads\\\\\"\n",
    "data_path=\"\\\\data\\\\03 - Trusted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main_path + data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pickle.load( open(r'Fuente_2_vista_minable.pickle', \"rb\" ) )\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar las columnas \n",
    "data2  = data2.rename(columns={\n",
    "     'year':                        \"periodo\", \n",
    "     \n",
    "     \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función general para preparación de datos: tokenización, remoción caracteres especiales\n",
    "# minusculas.... no incluido stemming y lematización.\n",
    "def textprep(line):\n",
    "    tokens = nltk.word_tokenize(str(line))\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=str(data2[\"mision_vision\"])\n",
    "tokenized_sent= sent_tokenize(text)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for t in text.split()]\n",
    "\n",
    "freq = nltk.FreqDist(tokens)\n",
    "\n",
    "\n",
    "for key,val in freq.items():\n",
    "\n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "\n",
    "sr = stopwords.words('spanish')\n",
    "\n",
    "for token in tokens:\n",
    "\n",
    "    if token in stopwords.words('spanish'):\n",
    "\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "\n",
    "sr = stopwords.words('spanish')\n",
    "\n",
    "for token in tokens:\n",
    "\n",
    "    if token in [\"La\", \"a\", \"o\", \"la\", \"del\", \"de\", \"es\", \"ser\", \"una\", \"infraestructura\",\"nacional\", \"como\", \"mision\",\"vision\" ]  :\n",
    "\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2 = nltk.FreqDist(clean_tokens)\n",
    "\n",
    "\n",
    "for key,val in freq.items():\n",
    "\n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "\n",
    "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(str(data2))\n",
    "df = pd.DataFrame()\n",
    "df['OriginalWords'] = pd.Series(words)\n",
    "#porter's stemmer\n",
    "porterStemmedWords = [nltk.stem.PorterStemmer().stem(word) for word in words]\n",
    "df['PorterStemmedWords'] = pd.Series(porterStemmedWords)\n",
    "#SnowBall stemmer\n",
    "snowballStemmedWords = [nltk.stem.SnowballStemmer(\"spanish\").stem(word) for word in words]\n",
    "df['SnowballStemmedWords'] = pd.Series(snowballStemmedWords)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem= WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"misiones\"\n",
    "print(\"lemmatized word:\", lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(str(data2))\n",
    "df = pd.DataFrame()\n",
    "df['OriginalWords'] = pd.Series(words)\n",
    "#WordNet Lemmatization\n",
    "wordNetLemmatizedWords = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "df['WordNetLemmatizer'] = pd.Series(wordNetLemmatizedWords)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función general para preparación de datos: tokenización, remoción caracteres especiales\n",
    "# minusculas.... no incluido stemming y lematización.\n",
    "def textprep(line):\n",
    "    tokens = nltk.word_tokenize(str(line))\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producto (nombre_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de columna con tokenización de una columna de interés especifica\n",
    "data2['tokens_nombre_producto'] = data2.apply(lambda row: textprep(row['nombre_producto']), axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(data2.tokens_nombre_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_nombre_producto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (corpus) #no correr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libreria para paralelizar\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "doc_term_matrix = pool.map(dictionary.doc2bow, [sentence for sentence in data2.tokens_nombre_producto])\n",
    "pool.close()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "t0 = time.time()\n",
    "lda_model = LdaMulticore(doc_term_matrix, num_topics=20, id2word = dictionary, passes=10, workers=10)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assigntopic(doc):\n",
    "    vector = lda_model[dictionary.doc2bow(doc)] \n",
    "    # opción 1: todos los tópicos ordenados de mayor a menor, podria ser topN tambien asi: return vector[:5] n=5\n",
    "    vector = sorted(vector, key=lambda item: -item[1])\n",
    "    # opción 2: asignar el tópico mayor a cada documento\n",
    "    #vector = max(vector,key=lambda item: item[1])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['topics'] = data2.apply(lambda row: assigntopic(row['tokens_nombre_producto']), axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos y sus pesos de un documento\n",
    "print(list(lda_model[doc_term_matrix[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos más relevantes de los tópicos más relevantes tópico y sus pesos\n",
    "print(lda_model.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda_model[corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensim\n",
    "import pyLDAvis\n",
    "\n",
    "t0 = time.time()\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics = False)\n",
    "print(time.time()-t0)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'lda_visualizatioproducto.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### modelo HDP con Producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "common_dictionary = Dictionary(data2.tokens_nombre_producto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "common_corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_nombre_producto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp = HdpModel(common_corpus, common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensim.prepare(hdp, common_corpus, common_dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data, 'hdp_visualizationproducto.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pesrpectiva estrategica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de columna con tokenización de una columna de interés especifica\n",
    "data2['tokens_pers_estrategica'] = data2.apply(lambda row: textprep(row['pers_estrategica']), axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el BoW (diccionario) de términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(data2.tokens_pers_estrategica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_pers_estrategica]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### no correr archivo pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Construir matriz de documentos vs términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libreria para paralelizar\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "doc_term_matrix = pool.map(dictionary.doc2bow, [sentence for sentence in data2.tokens_pers_estrategica])\n",
    "pool.close()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "t0 = time.time()\n",
    "lda_model = LdaMulticore(doc_term_matrix, num_topics=20, id2word = dictionary, passes=10, workers=10)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assigntopic(doc):\n",
    "    vector = lda_model[dictionary.doc2bow(doc)] \n",
    "    # opción 1: todos los tópicos ordenados de mayor a menor, podria ser topN tambien asi: return vector[:5] n=5\n",
    "    vector = sorted(vector, key=lambda item: -item[1])\n",
    "    # opción 2: asignar el tópico mayor a cada documento\n",
    "    #vector = max(vector,key=lambda item: item[1])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['topics2'] = data2.apply(lambda row: assigntopic(row['tokens_pers_estrategica']), axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos de tópicos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos y sus pesos de un documento\n",
    "print(list(lda_model[doc_term_matrix[0]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos más relevantes de los tópicos más relevantes tópico y sus pesos\n",
    "print(lda_model.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda_model[corpus]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de todos los tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensim\n",
    "import pyLDAvis\n",
    "\n",
    "t0 = time.time()\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics = False)\n",
    "print(time.time()-t0)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar la visualización en un archivo HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'lda_visualizationestrategica2.0.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HDP con Perpectiva Estrategica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(data2.tokens_pers_estrategica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "common_corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_pers_estrategica]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-085284fc0095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\hdpmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, id2word, max_chunks, max_time, chunksize, kappa, tau, K, T, alpha, gamma, eta, scale, var_converge, outputdir, random_state)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\hdpmodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm_num_docs_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m                 \u001b[0mchunks_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\hdpmodel.py\u001b[0m in \u001b[0;36mupdate_chunk\u001b[1;34m(self, chunk, update, opt_o)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m                 \u001b[0mdoc_word_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_word_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m                 doc_score = self.doc_e_step(\n\u001b[0m\u001b[0;32m    567\u001b[0m                     \u001b[0mss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mElogsticks_1st\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m                     \u001b[0munique_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_word_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\hdpmodel.py\u001b[0m in \u001b[0;36mdoc_e_step\u001b[1;34m(self, ss, Elogsticks_1st, unique_words, doc_word_ids, doc_word_counts, var_converge)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[1;31m# X part, the data part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m             \u001b[0mlikelihood\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_phi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mElogbeta_doc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdoc_word_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mconverge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlikelihood\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mold_likelihood\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_likelihood\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp = HdpModel(common_corpus, common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data2 = gensim.prepare(hdp, common_corpus, common_dictionary)\n",
    "pyLDAvis.display(vis_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data2, 'hdp_visualizationestrategica.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misión_visión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de columna con tokenización de una columna de interés especifica # 3 minutos y 30 segundos Cargando\n",
    "data2['tokens_mision_vision'] = data2.apply(lambda row: textprep(row['mision_vision']), axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(data2.tokens_mision_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_mision_vision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libreria para paralelizar\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "doc_term_matrix = pool.map(dictionary.doc2bow, [sentence for sentence in data2.tokens_mision_vision])\n",
    "pool.close()\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore \n",
    "#3 minutos cargando\n",
    "t0 = time.time()\n",
    "lda_model = LdaMulticore(doc_term_matrix, num_topics=20, id2word = dictionary, passes=10, workers=10)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assigntopic(doc):\n",
    "    vector = lda_model[dictionary.doc2bow(doc)] \n",
    "    # opción 1: todos los tópicos ordenados de mayor a menor, podria ser topN tambien asi: return vector[:5] n=5\n",
    "    vector = sorted(vector, key=lambda item: -item[1])\n",
    "    # opción 2: asignar el tópico mayor a cada documento\n",
    "    #vector = max(vector,key=lambda item: item[1])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['topics3'] = data2.apply(lambda row: assigntopic(row['tokens_mision_vision']), axis=1)\n",
    "data2.head()\n",
    "#1 minuto y medio cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos y sus pesos de un documento\n",
    "print(list(lda_model[doc_term_matrix[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los términos más relevantes de los tópicos más relevantes tópico y sus pesos\n",
    "print(lda_model.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in lda_model[corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensim\n",
    "import pyLDAvis\n",
    "\n",
    "t0 = time.time()\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics = False)\n",
    "print(time.time()-t0)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'lda_visualizationmisiovision2.0.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the HDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del BoW - en gensim es Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "common_dictionary = Dictionary(data2.tokens_mision_vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "common_corpus = [dictionary.doc2bow(doc) for doc in data2.tokens_mision_vision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp = HdpModel(common_corpus, common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data3 = gensim.prepare(hdp, common_corpus, common_dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data3, 'hdp_visualizationmisiovision.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
